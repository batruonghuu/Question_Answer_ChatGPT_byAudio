{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = open('key.txt','r').read().strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<OpenAIObject chat.completion id=chatcmpl-71Y0RqzWg7HUun0LR9ufOOBM3rneK at 0x23e3c6e8110> JSON: {\n  \"choices\": [\n    {\n      \"finish_reason\": \"stop\",\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"Hello! How can I assist you today?\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1680603403,\n  \"id\": \"chatcmpl-71Y0RqzWg7HUun0LR9ufOOBM3rneK\",\n  \"model\": \"gpt-3.5-turbo-0301\",\n  \"object\": \"chat.completion\",\n  \"usage\": {\n    \"completion_tokens\": 9,\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 18\n  }\n}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[{'role':'user','content':\"Hello\"}]\n",
    ")\n",
    "completion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello! How can I assist you today?'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message.content\n",
    "reply_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input was:\"  Hello, anyone here? \"\n"
     ]
    }
   ],
   "source": [
    "messages_history = []\n",
    "user_input = input(\">: \")\n",
    "print(\"User input was:\\\"\",user_input,'\\\"')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "messages_history.append({'role':'user','content':user_input})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hi there! How may I assist you today?'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages = messages_history,\n",
    ")\n",
    "reply_content = completion.choices[0].message.content\n",
    "reply_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "messages_history.append({'role':'assistant', 'content':reply_content})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'role': 'user', 'content': 'Hello, anyone here?'},\n {'role': 'assistant', 'content': 'Hi there! How may I assist you today?'}]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Hello! How can I assist you today?\n",
      "what today is?\n",
      "As an AI language model, I do not have access to real-time information about dates and times. However, if you let me know which time zone you are in, I could give you an approximate idea of what day and time it is in your location.\n"
     ]
    }
   ],
   "source": [
    "messages_history = []\n",
    "def chat(inp,role = 'user'):\n",
    "    messages_history.append({'role':role,'content':inp})\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=messages_history,\n",
    "    #     truyen messages_history de giup kha nang tu hoc cua chatGPT bot\n",
    "    )\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    # print(reply_content)\n",
    "    messages_history.append({'role':'assistant','content':reply_content})\n",
    "    return reply_content\n",
    "for i in range(2):\n",
    "    user_input = input(\">:\")\n",
    "    print(user_input)\n",
    "    print(chat(user_input))\n",
    "    # print(messages_history)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tf'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m p \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mautomatic-speech-recognition\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Study\\Python\\Question_Answer_ChatGPT_byAudio\\venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:739\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;66;03m# Use default model/config/tokenizer for the task if no model is provided\u001B[39;00m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    738\u001B[0m     \u001B[38;5;66;03m# At that point framework might still be undetermined\u001B[39;00m\n\u001B[1;32m--> 739\u001B[0m     model, default_revision \u001B[38;5;241m=\u001B[39m \u001B[43mget_default_model_and_revision\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtargeted_task\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframework\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    740\u001B[0m     revision \u001B[38;5;241m=\u001B[39m revision \u001B[38;5;28;01mif\u001B[39;00m revision \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m default_revision\n\u001B[0;32m    741\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    742\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo model was supplied, defaulted to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and revision\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    743\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    744\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a pipeline without specifying a model name and revision in production is not recommended.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    745\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Study\\Python\\Question_Answer_ChatGPT_byAudio\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:391\u001B[0m, in \u001B[0;36mget_default_model_and_revision\u001B[1;34m(targeted_task, framework, task_options)\u001B[0m\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m framework \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    389\u001B[0m     framework \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 391\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdefault_models\u001B[49m\u001B[43m[\u001B[49m\u001B[43mframework\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'tf'"
     ]
    }
   ],
   "source": [
    "p = pipeline(\"automatic-speech-recognition\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
